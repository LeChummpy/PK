{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)) )\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "def getinputStackedColumns(inputMatrix, kernel_map_shape, stride):\n",
    "    s0, s1, s2, s3 = inputMatrix.strides\n",
    "\n",
    "    d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_map_shape\n",
    "\n",
    "    out_shape = ( d1_input, d2_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "    inputStackedColumns = np.lib.stride_tricks.as_strided(inputMatrix,\n",
    "                                                          shape=out_shape,\n",
    "                                                          strides=(s0, s1, stride*s2,stride*s3,s2, s3))\n",
    "    return inputStackedColumns\n",
    "\n",
    "def getPadding(h_kernel, w_kernel, h_input, w_input, stride):\n",
    "    initial_h = h_input\n",
    "    initial_w = w_input \n",
    "    \n",
    "    while True:\n",
    "        if (h_input-h_kernel+1)%stride!=0:\n",
    "            h_input+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    while True:\n",
    "        w_input+=1\n",
    "        if (w_input-w_kernel+1)%stride!=0:\n",
    "            w_input+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "            \n",
    "    return h_input-initial_h, w_input-initial_w\n",
    "\n",
    "def Convolution_strided_img2col(inputMatrix, kernel_map, stride):\n",
    "  \n",
    "    d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    d_kernel, h_kernel, w_kernel = kernel_map.shape\n",
    "    \n",
    "    inputStackedColumns = getinputStackedColumns(inputMatrix, kernel_map.shape[1:], stride)\n",
    "    \n",
    "    out_shape = ( d1_input, d2_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    inputStackedColumns = inputStackedColumns.flatten()\n",
    "    inputStackedColumns = np.reshape(inputStackedColumns, (d1_input, d2_input, (h_input-h_kernel+1)//stride * ((w_input-w_kernel+1)//stride), h_kernel*w_kernel ))\n",
    "    kernel_map_edited = kernel_map.reshape(d_kernel, h_kernel*w_kernel).transpose()\n",
    "    im2col_conv = np.einsum(\"ijkl,lm->jkm\", inputStackedColumns, kernel_map_edited)\n",
    "    im2col_conv = im2col_conv.swapaxes(0,2).swapaxes(1,2)\n",
    "    im2col_conv = im2col_conv.reshape(im2col_conv.shape[0], im2col_conv.shape[1], out_shape[2], out_shape[3])\n",
    "    return im2col_conv\n",
    "\n",
    "def convolution(inputMatrix, kernel_map, stride):\n",
    "    #d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    #d_kernel, h_kernel, w_kernel = kernel_map.shape\n",
    "\n",
    "    #padding = getPadding(h_kernel, w_kernel, h_input, w_input, stride)\n",
    "    #inputMatrix = np.pad(inputMatrix, ((0,0),(0,0),(0,padding[0]),(0,padding[1])))\n",
    "    \n",
    "    return Convolution_strided_img2col(inputMatrix, kernel_map, stride)\n",
    "\n",
    "\n",
    "def Maxpooling(inputMatrix, kernel_shape, stride):\n",
    "\n",
    "    d_kernel_map, d_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_shape\n",
    "\n",
    "    windows = getinputStackedColumns(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    out_shape = ( d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    maxs = np.max(windows, axis=(4,5))\n",
    "    maxs = maxs.reshape(d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride)\n",
    "    return maxs\n",
    "\n",
    "def Averagepooling(inputMatrix, kernel_shape, stride):\n",
    "        \n",
    "    d_kernel_map, d_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_shape\n",
    "    \n",
    "    windows = getinputStackedColumns(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    out_shape = ( d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    means = np.means(windows, axis=(4,5))\n",
    "    means = means.reshape(d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride)\n",
    "    return means\n",
    "\n",
    "def pooling(inputMatrix, kernel_shape, stride, method):\n",
    "    if method==\"average\":\n",
    "        return Averagepooling(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    elif method==\"max\":\n",
    "        return Maxpooling(inputMatrix, kernel_shape, stride)\n",
    "\n",
    "    else:\n",
    "        return Maxpooling(inputMatrix, kernel_shape, stride)        \n",
    "\n",
    "def RELU_Matrixoperation(inputMatrix):\n",
    "    return np.maximum(inputMatrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    imgShape = None\n",
    "    convout1 = None \n",
    "    pooling1 = None\n",
    "    relu1 = None\n",
    "    convout2 = None \n",
    "    pooling2 = None\n",
    "    relu2 = None\n",
    "    x = None\n",
    "    a2 = None\n",
    "    a3 = None    \n",
    "    z2 = None\n",
    "    z3 = None\n",
    "    \n",
    "    w1 = None\n",
    "    w2 = None \n",
    "    kernelmap1 = None\n",
    "    kernelmap2 = None\n",
    "    pooling1kernelshape = None\n",
    "    pooling2kernelshape = None\n",
    "    \n",
    "    measurementData_Loss = []\n",
    "    measurementData_Accuracy = []\n",
    "    \n",
    "    def __init__(self, inputArrayShape, n_a2=50, n_a3=2, d_kernel1=5, w_h_kernel1=5, d_kernel2=3, w_h_kernel2=3, poolingmethod=\"max\", w_h_pooling1=2, w_h_pooling2=2, stride_conv1=1, stride_conv2=1, stride_pooling1=1, stride_pooling2=1, learningrate=0.1, batchsize=27):\n",
    "        self.imgShape = copy.copy(inputArrayShape)\n",
    "        self.poolingmethod = poolingmethod\n",
    "        self.stride_conv1 = stride_conv1 \n",
    "        self.stride_conv2 = stride_conv2 \n",
    "        self.stride_pooling1 = stride_pooling1\n",
    "        self.stride_pooling2 = stride_pooling2\n",
    "        self.learningrate = learningrate\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        self.pooling1kernelshape = (w_h_pooling1,w_h_pooling1)\n",
    "        self.pooling2kernelshape = (w_h_pooling2,w_h_pooling2)\n",
    "        \n",
    "        self.kernelmap1 = np.random.uniform(-1,1,(d_kernel1,w_h_kernel1,w_h_kernel1))\n",
    "        self.kernelmap2 = np.random.uniform(-1,1,(d_kernel2,w_h_kernel2,w_h_kernel2))\n",
    "        \n",
    "        inputArrayShape.insert(0, 1)\n",
    "        inputArrayShape.insert(0, 1) #input matrix: !one! example with depth !one!\n",
    "        n_x = self.getNumberOutputNeuronsConvolutionalLayer(inputArrayShape)\n",
    "        \n",
    "        self.w1 = np.random.uniform(-1,1,(n_a2, n_x+1))\n",
    "        self.w2 = np.random.uniform(-1,1,(n_a3, n_a2+1))\n",
    "        \n",
    "    def getNumberOutputNeuronsConvolutionalLayer(self, inputArrayShape):\n",
    "        self.input = np.zeros((inputArrayShape[0], inputArrayShape[1], inputArrayShape[2], inputArrayShape[3]))\n",
    "        self.__convolutional_layers(self.input)\n",
    "        return (self.x.shape[0])\n",
    "        \n",
    "    def __convolutional_layers(self, input):\n",
    "        self.convout1 = convolution(self.input, self.kernelmap1, self.stride_conv1) \n",
    "        self.reluout1 = RELU_Matrixoperation(self.convout1)\n",
    "        self.poolingout1 = pooling(self.reluout1, self.pooling1kernelshape, self.stride_pooling1, self.poolingmethod)\n",
    "\n",
    "        self.convout2 = convolution(self.poolingout1, self.kernelmap2, self.stride_conv2) \n",
    "        self.reluout2 = RELU_Matrixoperation(self.convout2)\n",
    "        self.poolingout2 = pooling(self.reluout2, self.pooling2kernelshape, self.stride_pooling2, self.poolingmethod)\n",
    "        \n",
    "        self.x = self.poolingout2.reshape(self.poolingout2.shape[0], self.poolingout2.shape[1], self.poolingout2.shape[2]*self.poolingout2.shape[3])\n",
    "        self.x = np.concatenate(self.x, axis=1).transpose()\n",
    "        \n",
    "    def propagateForward(self, input):\n",
    "        self.input = input.reshape(input.shape[0], 1, input.shape[1], input.shape[2]).swapaxes(0,1)\n",
    "        self.__convolutional_layers(self.input)\n",
    "        self.x = np.pad(self.x, ((1,0),(0,0)), constant_values=1)\n",
    "        \n",
    "        self.z2 = np.dot(self.w1, self.x)\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        self.a2 = np.pad(self.a2, ((1,0),(0,0)), constant_values=1)\n",
    "        \n",
    "        self.z3 = np.dot(self.w2, self.a2)\n",
    "        self.a3 = softmax(self.z3)\n",
    "        \n",
    "        return self.a3\n",
    "        \n",
    "        \n",
    "    def propagateBackward(self, input, y_hat):\n",
    "        self.propagateForward(input)\n",
    "        pd_L_wrt_a3 = 1/input.shape[0] * (self.a3 - y_hat)\n",
    "        pd_a3_wrt_z3 = softmax(self.z3)*(1 - softmax(self.z3))\n",
    "        pd_z3_wrt_w2 = self.a2\n",
    "        pd_L_wrt_w2 = np.dot(pd_L_wrt_a3 * pd_a3_wrt_z3, pd_z3_wrt_w2.T)\n",
    "        \n",
    "        pd_z3_wrt_a2 = self.w2\n",
    "        pd_a2_wrt_z2 = sigmoid(self.z2)*(1 - sigmoid(self.z2))\n",
    "        pd_z2_wrt_w1 = self.x\n",
    "        pd_L_wrt_w1 = np.dot( np.dot((pd_L_wrt_a3 * pd_a3_wrt_z3).T, pd_z3_wrt_a2[:,1:]).T * pd_a2_wrt_z2, pd_z2_wrt_w1.T)\n",
    "\n",
    "        self.w2 = self.w2 - self.learningrate * pd_L_wrt_w2\n",
    "        self.w1 = self.w1 - self.learningrate * pd_L_wrt_w1\n",
    "        \n",
    "    def getDataBatch(self, path, filenames):\n",
    "        uniform_size = self.imgShape[::-1]\n",
    "        \n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        \n",
    "        for i in filenames:\n",
    "            \n",
    "            img = image.imread(path + \"//\" + i)\n",
    "            if len(img.shape)==3:\n",
    "                img = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "            resized_img = cv2.resize(img, dsize=uniform_size, interpolation=cv2.INTER_CUBIC)\n",
    "            data_x.append(resized_img)\n",
    "\n",
    "\n",
    "            y = None\n",
    "            if \"y\" in i:\n",
    "                y = np.array([1,0])\n",
    "            else:\n",
    "                y = np.array([0,1])\n",
    "            data_y.append(y)\n",
    "            \n",
    "        data_x = np.array(data_x).astype(np.float)  \n",
    "        data_y = np.array(data_y).astype(np.float).transpose()\n",
    "        \n",
    "        return(data_x, data_y)\n",
    "        \n",
    "    def train(self, epochs, trainpath, testpath):\n",
    "        \n",
    "        trainfilenames = os.listdir(trainpath)\n",
    "        testfilenames = os.listdir(testpath)\n",
    "        \n",
    "        np.random.shuffle(trainfilenames)\n",
    "        np.random.shuffle(testfilenames)\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for i in range(epochs):\n",
    "            filenamesBatch = trainfilenames[batch_counter:batch_counter+self.batchsize]\n",
    "            \n",
    "            data = self.getDataBatch(trainpath, filenamesBatch)\n",
    "            input = data[0]\n",
    "            y_hat = data[1]\n",
    "            self.propagateBackward(input, y_hat)\n",
    "            loss = 1/input.shape[0] * ((self.a3 - y_hat)**2).sum()\n",
    "            \n",
    "            data_test = self.getDataBatch(testpath, testfilenames[:100])\n",
    "            accuracy = self.test(data_test[0], data_test[1])\n",
    "            print(\"Epoch \" + str(i) + \" done. Loss: \" + str(loss) + \" Accuracy: \" + str(accuracy*100) + \" %\")\n",
    "            self.measurementData_Accuracy.append(accuracy)\n",
    "            self.measurementData_Loss.append(loss)\n",
    "            \n",
    "            batch_counter += self.batchsize\n",
    "            if (batch_counter>=2700-self.batchsize):\n",
    "                batch_counter = 0\n",
    "            \n",
    "    def test(self, input, y_hat):\n",
    "        fx = self.propagateForward(input)\n",
    "        number_correct = 0\n",
    "        number_overall = fx[:,:100].shape[1]\n",
    "        for i in range(number_overall):\n",
    "            arrayfx = fx[:,i:i+1]\n",
    "            arrayy = y_hat[:,i:i+1]\n",
    "           \n",
    "            indexmaxfx = np.argmax(arrayfx, axis=0)\n",
    "            indexmaxy = np.argmax(arrayy, axis=0)\n",
    "            if (indexmaxfx[0]==indexmaxy[0]):\n",
    "                number_correct += 1\n",
    "                \n",
    "        return number_correct/number_overall\n",
    "    \n",
    "    def getMeasurementData(self):\n",
    "        return (self.measurementData_Loss, self.measurementData_Accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:132: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:133: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done. Loss: 0.9629629249187502 Accuracy: 45.0 %\n",
      "Epoch 1 done. Loss: 0.9068388585705159 Accuracy: 45.0 %\n"
     ]
    }
   ],
   "source": [
    "m = Model([175, 167], n_a2=1000, n_a3=2, d_kernel1=5, w_h_kernel1=5, d_kernel2=5, w_h_kernel2=5, poolingmethod=\"max\", w_h_pooling1=3, w_h_pooling2=3, stride_conv1=2, stride_conv2=2, stride_pooling1=1, stride_pooling2=1, learningrate=0.5, batchsize=27)\n",
    "m.train(500, \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//train\", \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_data_models = []\n",
    "tracking_data_accuracy = []\n",
    "\n",
    "while True:\n",
    "    n_a2 = randint(1,50)*1000\n",
    "    n_a3 = 2\n",
    "    d_kernel1 = randint(1,10)*2\n",
    "    w_h_kernel1 = randint(1,6)\n",
    "    d_kernel1 = randint(1,10)*2\n",
    "    w_h_kernel2 = randint(1,6)\n",
    "    poolingmethod = randint(1,2) #1--> max, 2--> average\n",
    "    w_h_pooling1 = randint(1,10)\n",
    "    w_h_pooling2 = randint(1,10)\n",
    "    stride_conv1 =\n",
    "    stride_conv2 =\n",
    "    stride_pooling1 =\n",
    "    stride_pooling2 =\n",
    "    learningrate = 0.5\n",
    "    batchsize = 27\n",
    "    \n",
    "    m = Model([175, 167], n_a2, n_a3, d_kernel1, w_h_kernel1, d_kernel2, w_h_kernel2, poolingmethod=, w_h_pooling1, w_h_pooling2, stride_conv1, stride_conv2, stride_pooling1, stride_pooling2, learningrate, batchsize)\n",
    "    m.train(500, \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//train\", \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
