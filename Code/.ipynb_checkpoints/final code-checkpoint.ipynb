{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)) )\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "def getinputStackedColumns(inputMatrix, kernel_map_shape, stride):\n",
    "    s0, s1, s2, s3 = inputMatrix.strides\n",
    "\n",
    "    d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_map_shape\n",
    "\n",
    "    out_shape = ( d1_input, d2_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "    inputStackedColumns = np.lib.stride_tricks.as_strided(inputMatrix,\n",
    "                                                          shape=out_shape,\n",
    "                                                          strides=(s0, s1, stride*s2,stride*s3,s2, s3))\n",
    "    return inputStackedColumns\n",
    "\n",
    "def getPadding(h_kernel, w_kernel, h_input, w_input, stride):\n",
    "    initial_h = h_input\n",
    "    initial_w = w_input \n",
    "    \n",
    "    while True:\n",
    "        if (h_input-h_kernel+1)%stride!=0:\n",
    "            h_input+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    while True:\n",
    "        w_input+=1\n",
    "        if (w_input-w_kernel+1)%stride!=0:\n",
    "            w_input+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "            \n",
    "    return h_input-initial_h, w_input-initial_w\n",
    "\n",
    "def Convolution_strided_img2col(inputMatrix, kernel_map, stride):\n",
    "  \n",
    "    d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    d_kernel, h_kernel, w_kernel = kernel_map.shape\n",
    "    \n",
    "    inputStackedColumns = getinputStackedColumns(inputMatrix, kernel_map.shape[1:], stride)\n",
    "    \n",
    "    out_shape = ( d1_input, d2_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    inputStackedColumns = inputStackedColumns.flatten()\n",
    "    inputStackedColumns = np.reshape(inputStackedColumns, (d1_input, d2_input, (h_input-h_kernel+1)//stride * ((w_input-w_kernel+1)//stride), h_kernel*w_kernel ))\n",
    "    kernel_map_edited = kernel_map.reshape(d_kernel, h_kernel*w_kernel).transpose()\n",
    "    im2col_conv = np.einsum(\"ijkl,lm->jkm\", inputStackedColumns, kernel_map_edited)\n",
    "    im2col_conv = im2col_conv.swapaxes(0,2).swapaxes(1,2)\n",
    "    im2col_conv = im2col_conv.reshape(im2col_conv.shape[0], im2col_conv.shape[1], out_shape[2], out_shape[3])\n",
    "    return im2col_conv\n",
    "\n",
    "def convolution(inputMatrix, kernel_map, stride):\n",
    "    #d1_input, d2_input, h_input, w_input = inputMatrix.shape\n",
    "    #d_kernel, h_kernel, w_kernel = kernel_map.shape\n",
    "\n",
    "    #padding = getPadding(h_kernel, w_kernel, h_input, w_input, stride)\n",
    "    #inputMatrix = np.pad(inputMatrix, ((0,0),(0,0),(0,padding[0]),(0,padding[1])))\n",
    "    \n",
    "    return Convolution_strided_img2col(inputMatrix, kernel_map, stride)\n",
    "\n",
    "\n",
    "def Maxpooling(inputMatrix, kernel_shape, stride):\n",
    "\n",
    "    d_kernel_map, d_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_shape\n",
    "\n",
    "    windows = getinputStackedColumns(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    out_shape = ( d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    maxs = np.max(windows, axis=(4,5))\n",
    "    maxs = maxs.reshape(d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride)\n",
    "    return maxs\n",
    "\n",
    "def Averagepooling(inputMatrix, kernel_shape, stride):\n",
    "        \n",
    "    d_kernel_map, d_input, h_input, w_input = inputMatrix.shape\n",
    "    h_kernel, w_kernel = kernel_shape\n",
    "    \n",
    "    windows = getinputStackedColumns(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    out_shape = ( d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride, h_kernel, w_kernel)\n",
    "\n",
    "    means = np.means(windows, axis=(4,5))\n",
    "    means = means.reshape(d_kernel_map, d_input, (h_input-h_kernel+1)//stride, (w_input-w_kernel+1)//stride)\n",
    "    return means\n",
    "\n",
    "def pooling(inputMatrix, kernel_shape, stride, method):\n",
    "    if method==\"average\":\n",
    "        return Averagepooling(inputMatrix, kernel_shape, stride)\n",
    "    \n",
    "    elif method==\"max\":\n",
    "        return Maxpooling(inputMatrix, kernel_shape, stride)\n",
    "\n",
    "    else:\n",
    "        return Maxpooling(inputMatrix, kernel_shape, stride)        \n",
    "\n",
    "def RELU_Matrixoperation(inputMatrix):\n",
    "    return np.maximum(inputMatrix, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    imgShape = None\n",
    "    convout1 = None \n",
    "    pooling1 = None\n",
    "    relu1 = None\n",
    "    convout2 = None \n",
    "    pooling2 = None\n",
    "    relu2 = None\n",
    "    x = None\n",
    "    a2 = None\n",
    "    a3 = None    \n",
    "    z2 = None\n",
    "    z3 = None\n",
    "    \n",
    "    w1 = None\n",
    "    w2 = None \n",
    "    kernelmap1 = None\n",
    "    kernelmap2 = None\n",
    "    pooling1kernelshape = None\n",
    "    pooling2kernelshape = None\n",
    "    \n",
    "    measurementData_Loss = []\n",
    "    measurementData_Accuracy = []\n",
    "    \n",
    "    def __init__(self, inputArrayShape, n_a2=50, n_a3=2, d_kernel1=5, w_h_kernel1=5, d_kernel2=3, w_h_kernel2=3, poolingmethod=\"max\", w_h_pooling1=2, w_h_pooling2=2, stride_conv1=1, stride_conv2=1, stride_pooling1=1, stride_pooling2=1, learningrate=0.1, batchsize=27):\n",
    "        self.imgShape = copy.copy(inputArrayShape)\n",
    "        self.poolingmethod = poolingmethod\n",
    "        self.stride_conv1 = stride_conv1 \n",
    "        self.stride_conv2 = stride_conv2 \n",
    "        self.stride_pooling1 = stride_pooling1\n",
    "        self.stride_pooling2 = stride_pooling2\n",
    "        self.learningrate = learningrate\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        self.pooling1kernelshape = (w_h_pooling1,w_h_pooling1)\n",
    "        self.pooling2kernelshape = (w_h_pooling2,w_h_pooling2)\n",
    "        \n",
    "        self.kernelmap1 = np.random.uniform(-1,1,(d_kernel1,w_h_kernel1,w_h_kernel1))\n",
    "        self.kernelmap2 = np.random.uniform(-1,1,(d_kernel2,w_h_kernel2,w_h_kernel2))\n",
    "        \n",
    "        inputArrayShape.insert(0, 1)\n",
    "        inputArrayShape.insert(0, 1) #input matrix: !one! example with depth !one!\n",
    "        n_x = self.getNumberOutputNeuronsConvolutionalLayer(inputArrayShape)\n",
    "        \n",
    "        self.w1 = np.random.uniform(-1,1,(n_a2, n_x+1))\n",
    "        self.w2 = np.random.uniform(-1,1,(n_a3, n_a2+1))\n",
    "        \n",
    "    def getNumberOutputNeuronsConvolutionalLayer(self, inputArrayShape):\n",
    "        self.input = np.zeros((inputArrayShape[0], inputArrayShape[1], inputArrayShape[2], inputArrayShape[3]))\n",
    "        self.__convolutional_layers(self.input)\n",
    "        return (self.x.shape[0])\n",
    "        \n",
    "    def __convolutional_layers(self, input):\n",
    "        self.convout1 = convolution(self.input, self.kernelmap1, self.stride_conv1) \n",
    "        self.reluout1 = RELU_Matrixoperation(self.convout1)\n",
    "        self.poolingout1 = pooling(self.reluout1, self.pooling1kernelshape, self.stride_pooling1, self.poolingmethod)\n",
    "\n",
    "        self.convout2 = convolution(self.poolingout1, self.kernelmap2, self.stride_conv2) \n",
    "        self.reluout2 = RELU_Matrixoperation(self.convout2)\n",
    "        self.poolingout2 = pooling(self.reluout2, self.pooling2kernelshape, self.stride_pooling2, self.poolingmethod)\n",
    "        \n",
    "        self.x = self.poolingout2.reshape(self.poolingout2.shape[0], self.poolingout2.shape[1], self.poolingout2.shape[2]*self.poolingout2.shape[3])\n",
    "        self.x = np.concatenate(self.x, axis=1).transpose()\n",
    "        \n",
    "    def propagateForward(self, input):\n",
    "        self.input = input.reshape(input.shape[0], 1, input.shape[1], input.shape[2]).swapaxes(0,1)\n",
    "        self.__convolutional_layers(self.input)\n",
    "        self.x = np.pad(self.x, ((1,0),(0,0)), constant_values=1)\n",
    "        \n",
    "        self.z2 = np.dot(self.w1, self.x)\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        self.a2 = np.pad(self.a2, ((1,0),(0,0)), constant_values=1)\n",
    "        \n",
    "        self.z3 = np.dot(self.w2, self.a2)\n",
    "        self.a3 = softmax(self.z3)\n",
    "        \n",
    "        return self.a3\n",
    "        \n",
    "        \n",
    "    def propagateBackward(self, input, y_hat):\n",
    "        self.propagateForward(input)\n",
    "        pd_L_wrt_a3 = 1/input.shape[0] * (self.a3 - y_hat)\n",
    "        pd_a3_wrt_z3 = softmax(self.z3)*(1 - softmax(self.z3))\n",
    "        pd_z3_wrt_w2 = self.a2\n",
    "        pd_L_wrt_w2 = np.dot(pd_L_wrt_a3 * pd_a3_wrt_z3, pd_z3_wrt_w2.T)\n",
    "        \n",
    "        pd_z3_wrt_a2 = self.w2\n",
    "        pd_a2_wrt_z2 = sigmoid(self.z2)*(1 - sigmoid(self.z2))\n",
    "        pd_z2_wrt_w1 = self.x\n",
    "        pd_L_wrt_w1 = np.dot( np.dot((pd_L_wrt_a3 * pd_a3_wrt_z3).T, pd_z3_wrt_a2[:,1:]).T * pd_a2_wrt_z2, pd_z2_wrt_w1.T)\n",
    "\n",
    "        self.w2 = self.w2 - self.learningrate * pd_L_wrt_w2\n",
    "        self.w1 = self.w1 - self.learningrate * pd_L_wrt_w1\n",
    "        \n",
    "    def getDataBatch(self, path, filenames):\n",
    "        uniform_size = self.imgShape[::-1]\n",
    "        \n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        \n",
    "        for i in filenames:\n",
    "            \n",
    "            img = image.imread(path + \"//\" + i)\n",
    "            if len(img.shape)==3:\n",
    "                img = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "            resized_img = cv2.resize(img, dsize=uniform_size, interpolation=cv2.INTER_CUBIC)\n",
    "            data_x.append(resized_img)\n",
    "\n",
    "\n",
    "            y = None\n",
    "            if \"y\" in i:\n",
    "                y = np.array([1,0])\n",
    "            else:\n",
    "                y = np.array([0,1])\n",
    "            data_y.append(y)\n",
    "            \n",
    "        data_x = np.array(data_x).astype(np.float)  \n",
    "        data_y = np.array(data_y).astype(np.float).transpose()\n",
    "        \n",
    "        return(data_x, data_y)\n",
    "        \n",
    "    def train(self, epochs, trainpath, testpath):\n",
    "        \n",
    "        trainfilenames = os.listdir(trainpath)\n",
    "        testfilenames = os.listdir(testpath)\n",
    "        \n",
    "        np.random.shuffle(trainfilenames)\n",
    "        np.random.shuffle(testfilenames)\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for i in range(epochs):\n",
    "            filenamesBatch = trainfilenames[batch_counter:batch_counter+self.batchsize]\n",
    "            \n",
    "            data = self.getDataBatch(trainpath, filenamesBatch)\n",
    "            input = data[0]\n",
    "            y_hat = data[1]\n",
    "            self.propagateBackward(input, y_hat)\n",
    "            loss = 1/input.shape[0] * ((self.a3 - y_hat)**2).sum()\n",
    "            \n",
    "            data_test = self.getDataBatch(testpath, testfilenames[:100])\n",
    "            accuracy = self.test(data_test[0], data_test[1])\n",
    "            print(\"Epoch \" + str(i) + \" done. Loss: \" + str(loss) + \" Accuracy: \" + str(accuracy*100) + \" %\")\n",
    "            self.measurementData_Accuracy.append(accuracy)\n",
    "            self.measurementData_Loss.append(loss)\n",
    "            \n",
    "            batch_counter += self.batchsize\n",
    "            if (batch_counter>=2700-self.batchsize):\n",
    "                batch_counter = 0\n",
    "            \n",
    "    def test(self, input, y_hat):\n",
    "        fx = self.propagateForward(input)\n",
    "        number_correct = 0\n",
    "        number_overall = fx[:,:100].shape[1]\n",
    "        for i in range(number_overall):\n",
    "            arrayfx = fx[:,i:i+1]\n",
    "            arrayy = y_hat[:,i:i+1]\n",
    "           \n",
    "            indexmaxfx = np.argmax(arrayfx, axis=0)\n",
    "            indexmaxy = np.argmax(arrayy, axis=0)\n",
    "            if (indexmaxfx[0]==indexmaxy[0]):\n",
    "                number_correct += 1\n",
    "                \n",
    "        return number_correct/number_overall\n",
    "    \n",
    "    def getMeasurementData(self):\n",
    "        return (self.measurementData_Loss, self.measurementData_Accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "c:\\users\\benni\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done. Loss: 0.8799924841583684 Accuracy: 47.0 %\n",
      "Epoch 1 done. Loss: 1.2441167996517393 Accuracy: 46.0 %\n",
      "Epoch 2 done. Loss: 1.1229090205089423 Accuracy: 49.0 %\n",
      "Epoch 3 done. Loss: 0.9899639850737612 Accuracy: 47.0 %\n",
      "Epoch 4 done. Loss: 0.8808706628136197 Accuracy: 47.0 %\n",
      "Epoch 5 done. Loss: 1.1108695305293463 Accuracy: 47.0 %\n",
      "Epoch 6 done. Loss: 0.5194664481520674 Accuracy: 47.0 %\n",
      "Epoch 7 done. Loss: 0.574894681117785 Accuracy: 50.0 %\n",
      "Epoch 8 done. Loss: 0.957264814613847 Accuracy: 49.0 %\n",
      "Epoch 9 done. Loss: 1.2701698605149345 Accuracy: 48.0 %\n",
      "Epoch 10 done. Loss: 1.0539996269453462 Accuracy: 52.0 %\n",
      "Epoch 11 done. Loss: 0.7989674110303563 Accuracy: 52.0 %\n",
      "Epoch 12 done. Loss: 0.7060314899227225 Accuracy: 56.00000000000001 %\n",
      "Epoch 13 done. Loss: 0.7985726409578651 Accuracy: 56.00000000000001 %\n",
      "Epoch 14 done. Loss: 0.6984668791084903 Accuracy: 50.0 %\n",
      "Epoch 15 done. Loss: 0.793144174323534 Accuracy: 51.0 %\n",
      "Epoch 16 done. Loss: 0.8888729908094906 Accuracy: 51.0 %\n",
      "Epoch 17 done. Loss: 0.7693866625197413 Accuracy: 51.0 %\n",
      "Epoch 18 done. Loss: 0.9793613491554605 Accuracy: 49.0 %\n",
      "Epoch 19 done. Loss: 1.0155466024253 Accuracy: 53.0 %\n",
      "Epoch 20 done. Loss: 1.0450086968820294 Accuracy: 62.0 %\n",
      "Epoch 21 done. Loss: 0.779881221624261 Accuracy: 56.99999999999999 %\n",
      "Epoch 22 done. Loss: 0.8485895943390732 Accuracy: 60.0 %\n",
      "Epoch 23 done. Loss: 0.5658134220181004 Accuracy: 56.99999999999999 %\n",
      "Epoch 24 done. Loss: 0.6919274342045012 Accuracy: 63.0 %\n",
      "Epoch 25 done. Loss: 0.7199083522929098 Accuracy: 63.0 %\n",
      "Epoch 26 done. Loss: 0.44644311584524216 Accuracy: 65.0 %\n",
      "Epoch 27 done. Loss: 0.5806186587510219 Accuracy: 61.0 %\n",
      "Epoch 28 done. Loss: 0.3654527559886317 Accuracy: 60.0 %\n",
      "Epoch 29 done. Loss: 0.5240996920923382 Accuracy: 59.0 %\n",
      "Epoch 30 done. Loss: 0.629805843373862 Accuracy: 60.0 %\n",
      "Epoch 31 done. Loss: 0.6281202104149192 Accuracy: 53.0 %\n",
      "Epoch 32 done. Loss: 0.7299816179185645 Accuracy: 57.99999999999999 %\n",
      "Epoch 33 done. Loss: 0.939898014279913 Accuracy: 59.0 %\n",
      "Epoch 34 done. Loss: 0.8444719972672027 Accuracy: 65.0 %\n",
      "Epoch 35 done. Loss: 0.7317935000162515 Accuracy: 64.0 %\n",
      "Epoch 36 done. Loss: 0.6598530155825398 Accuracy: 50.0 %\n",
      "Epoch 37 done. Loss: 0.6573060440751142 Accuracy: 52.0 %\n",
      "Epoch 38 done. Loss: 0.6918104178421818 Accuracy: 59.0 %\n",
      "Epoch 39 done. Loss: 0.7761740877818541 Accuracy: 67.0 %\n",
      "Epoch 40 done. Loss: 0.5042286974928941 Accuracy: 65.0 %\n",
      "Epoch 41 done. Loss: 0.4634025104930006 Accuracy: 59.0 %\n",
      "Epoch 42 done. Loss: 1.0322373878573519 Accuracy: 59.0 %\n",
      "Epoch 43 done. Loss: 0.6521650703519787 Accuracy: 63.0 %\n",
      "Epoch 44 done. Loss: 0.6321293575129661 Accuracy: 67.0 %\n",
      "Epoch 45 done. Loss: 0.512063742175738 Accuracy: 68.0 %\n",
      "Epoch 46 done. Loss: 0.588767922349728 Accuracy: 63.0 %\n",
      "Epoch 47 done. Loss: 0.47699090796346444 Accuracy: 68.0 %\n",
      "Epoch 48 done. Loss: 0.6677708555051002 Accuracy: 69.0 %\n",
      "Epoch 49 done. Loss: 0.8774169873569758 Accuracy: 68.0 %\n",
      "Epoch 50 done. Loss: 0.4895498448694598 Accuracy: 64.0 %\n",
      "Epoch 51 done. Loss: 0.3432455716223901 Accuracy: 67.0 %\n",
      "Epoch 52 done. Loss: 0.58716989722042 Accuracy: 69.0 %\n",
      "Epoch 53 done. Loss: 0.4656658446104818 Accuracy: 70.0 %\n",
      "Epoch 54 done. Loss: 0.5178612268893543 Accuracy: 69.0 %\n",
      "Epoch 55 done. Loss: 0.3795476256046138 Accuracy: 68.0 %\n",
      "Epoch 56 done. Loss: 1.0438415722840453 Accuracy: 70.0 %\n",
      "Epoch 57 done. Loss: 0.680961386504549 Accuracy: 69.0 %\n",
      "Epoch 58 done. Loss: 0.39063858279963454 Accuracy: 70.0 %\n",
      "Epoch 59 done. Loss: 0.614260055924542 Accuracy: 65.0 %\n",
      "Epoch 60 done. Loss: 0.6666386909785725 Accuracy: 65.0 %\n",
      "Epoch 61 done. Loss: 0.7432968768584615 Accuracy: 64.0 %\n",
      "Epoch 62 done. Loss: 0.9287888286316244 Accuracy: 59.0 %\n",
      "Epoch 63 done. Loss: 0.9532992069188113 Accuracy: 71.0 %\n",
      "Epoch 64 done. Loss: 0.3354715852107056 Accuracy: 61.0 %\n",
      "Epoch 65 done. Loss: 0.7826027121047516 Accuracy: 66.0 %\n",
      "Epoch 66 done. Loss: 0.40907710666964087 Accuracy: 59.0 %\n",
      "Epoch 67 done. Loss: 1.0613383838021038 Accuracy: 70.0 %\n",
      "Epoch 68 done. Loss: 0.66504502040491 Accuracy: 71.0 %\n",
      "Epoch 69 done. Loss: 0.44400362966583506 Accuracy: 70.0 %\n",
      "Epoch 70 done. Loss: 0.4973571561192375 Accuracy: 72.0 %\n",
      "Epoch 71 done. Loss: 0.49726964522966494 Accuracy: 71.0 %\n",
      "Epoch 72 done. Loss: 0.4921052506567254 Accuracy: 72.0 %\n",
      "Epoch 73 done. Loss: 0.829435339642941 Accuracy: 69.0 %\n",
      "Epoch 74 done. Loss: 0.2849708429695463 Accuracy: 67.0 %\n",
      "Epoch 75 done. Loss: 0.472025321339253 Accuracy: 71.0 %\n",
      "Epoch 76 done. Loss: 0.6886378591297215 Accuracy: 67.0 %\n",
      "Epoch 77 done. Loss: 0.462627836062527 Accuracy: 68.0 %\n",
      "Epoch 78 done. Loss: 0.3467459128988084 Accuracy: 71.0 %\n",
      "Epoch 79 done. Loss: 0.6612019852439706 Accuracy: 70.0 %\n",
      "Epoch 80 done. Loss: 0.49931089655312144 Accuracy: 73.0 %\n",
      "Epoch 81 done. Loss: 0.5415089146223289 Accuracy: 74.0 %\n",
      "Epoch 82 done. Loss: 0.3014243826524392 Accuracy: 76.0 %\n",
      "Epoch 83 done. Loss: 0.6645424685302915 Accuracy: 76.0 %\n",
      "Epoch 84 done. Loss: 0.3889473127651061 Accuracy: 72.0 %\n",
      "Epoch 85 done. Loss: 0.5472934263871597 Accuracy: 61.0 %\n",
      "Epoch 86 done. Loss: 0.8105839622089194 Accuracy: 66.0 %\n",
      "Epoch 87 done. Loss: 0.5915733008722343 Accuracy: 72.0 %\n",
      "Epoch 88 done. Loss: 0.7806928028980782 Accuracy: 73.0 %\n",
      "Epoch 89 done. Loss: 0.5778101909391423 Accuracy: 72.0 %\n",
      "Epoch 90 done. Loss: 0.7690523700985094 Accuracy: 72.0 %\n",
      "Epoch 91 done. Loss: 0.5289341451912946 Accuracy: 74.0 %\n",
      "Epoch 92 done. Loss: 0.8561026574796864 Accuracy: 77.0 %\n",
      "Epoch 93 done. Loss: 0.5368208076990585 Accuracy: 74.0 %\n",
      "Epoch 94 done. Loss: 0.5871020399127356 Accuracy: 75.0 %\n",
      "Epoch 95 done. Loss: 0.20375307205273777 Accuracy: 71.0 %\n",
      "Epoch 96 done. Loss: 0.7569429378885474 Accuracy: 72.0 %\n",
      "Epoch 97 done. Loss: 0.4989559736483289 Accuracy: 71.0 %\n",
      "Epoch 98 done. Loss: 0.29651474477611905 Accuracy: 72.0 %\n",
      "Epoch 99 done. Loss: 0.598732886491772 Accuracy: 73.0 %\n",
      "Epoch 100 done. Loss: 0.4467975472155583 Accuracy: 73.0 %\n",
      "Epoch 101 done. Loss: 0.36939277552259486 Accuracy: 73.0 %\n",
      "Epoch 102 done. Loss: 0.8345319367439796 Accuracy: 78.0 %\n",
      "Epoch 103 done. Loss: 0.4800268725716476 Accuracy: 72.0 %\n",
      "Epoch 104 done. Loss: 0.45821975319765273 Accuracy: 75.0 %\n",
      "Epoch 105 done. Loss: 0.6696581597880674 Accuracy: 76.0 %\n",
      "Epoch 106 done. Loss: 0.41547948003028545 Accuracy: 76.0 %\n",
      "Epoch 107 done. Loss: 0.512896876790036 Accuracy: 67.0 %\n",
      "Epoch 108 done. Loss: 1.000843583359294 Accuracy: 72.0 %\n",
      "Epoch 109 done. Loss: 0.7702871781518109 Accuracy: 71.0 %\n",
      "Epoch 110 done. Loss: 0.6568892142145893 Accuracy: 73.0 %\n",
      "Epoch 111 done. Loss: 0.8361043199501594 Accuracy: 73.0 %\n",
      "Epoch 112 done. Loss: 0.8773561174365584 Accuracy: 72.0 %\n",
      "Epoch 113 done. Loss: 0.6685620136231002 Accuracy: 78.0 %\n",
      "Epoch 114 done. Loss: 0.7024067617787508 Accuracy: 74.0 %\n",
      "Epoch 115 done. Loss: 0.6255667754984291 Accuracy: 75.0 %\n",
      "Epoch 116 done. Loss: 0.6011121764889241 Accuracy: 72.0 %\n",
      "Epoch 117 done. Loss: 0.4516269437530758 Accuracy: 72.0 %\n",
      "Epoch 118 done. Loss: 0.5338524025596544 Accuracy: 77.0 %\n",
      "Epoch 119 done. Loss: 0.5402420679157962 Accuracy: 72.0 %\n",
      "Epoch 120 done. Loss: 0.7582013040596935 Accuracy: 71.0 %\n",
      "Epoch 121 done. Loss: 0.7343744167283409 Accuracy: 66.0 %\n",
      "Epoch 122 done. Loss: 0.34782539757833075 Accuracy: 78.0 %\n",
      "Epoch 123 done. Loss: 0.31460820110957166 Accuracy: 73.0 %\n",
      "Epoch 124 done. Loss: 0.558229224816703 Accuracy: 66.0 %\n",
      "Epoch 125 done. Loss: 0.5989247774699795 Accuracy: 66.0 %\n",
      "Epoch 126 done. Loss: 0.5301835569409646 Accuracy: 73.0 %\n",
      "Epoch 127 done. Loss: 0.07408372312338302 Accuracy: 73.0 %\n",
      "Epoch 128 done. Loss: 0.48492726088246507 Accuracy: 77.0 %\n",
      "Epoch 129 done. Loss: 0.5632006608606293 Accuracy: 72.0 %\n",
      "Epoch 130 done. Loss: 0.5415762851913434 Accuracy: 72.0 %\n",
      "Epoch 131 done. Loss: 0.5138408777916754 Accuracy: 72.0 %\n",
      "Epoch 132 done. Loss: 0.6147864498877064 Accuracy: 66.0 %\n",
      "Epoch 133 done. Loss: 0.5764794005468887 Accuracy: 73.0 %\n",
      "Epoch 134 done. Loss: 0.5181626985999962 Accuracy: 73.0 %\n",
      "Epoch 135 done. Loss: 0.5426863840290278 Accuracy: 76.0 %\n",
      "Epoch 136 done. Loss: 0.43799397362706094 Accuracy: 74.0 %\n",
      "Epoch 137 done. Loss: 0.4044881017200601 Accuracy: 69.0 %\n",
      "Epoch 138 done. Loss: 0.6209290673429947 Accuracy: 68.0 %\n",
      "Epoch 139 done. Loss: 0.39979347962385287 Accuracy: 76.0 %\n",
      "Epoch 140 done. Loss: 0.39542560028369783 Accuracy: 77.0 %\n",
      "Epoch 141 done. Loss: 0.6595000580948429 Accuracy: 78.0 %\n",
      "Epoch 142 done. Loss: 0.295947247602407 Accuracy: 78.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143 done. Loss: 0.6127570112387148 Accuracy: 70.0 %\n",
      "Epoch 144 done. Loss: 0.5574734418162987 Accuracy: 74.0 %\n",
      "Epoch 145 done. Loss: 0.26885435794785795 Accuracy: 74.0 %\n",
      "Epoch 146 done. Loss: 0.27995648596778083 Accuracy: 76.0 %\n",
      "Epoch 147 done. Loss: 0.40310096795734107 Accuracy: 77.0 %\n",
      "Epoch 148 done. Loss: 0.624730026349177 Accuracy: 73.0 %\n",
      "Epoch 149 done. Loss: 0.3942957904626451 Accuracy: 73.0 %\n",
      "Epoch 150 done. Loss: 0.27330135032459 Accuracy: 79.0 %\n",
      "Epoch 151 done. Loss: 0.5507117761396618 Accuracy: 74.0 %\n",
      "Epoch 152 done. Loss: 0.39244944520831954 Accuracy: 74.0 %\n",
      "Epoch 153 done. Loss: 0.5229379112338249 Accuracy: 74.0 %\n",
      "Epoch 154 done. Loss: 0.36935786424743644 Accuracy: 74.0 %\n",
      "Epoch 155 done. Loss: 0.7581332220619281 Accuracy: 78.0 %\n",
      "Epoch 156 done. Loss: 0.6748290786437915 Accuracy: 76.0 %\n",
      "Epoch 157 done. Loss: 0.4235456402687506 Accuracy: 79.0 %\n",
      "Epoch 158 done. Loss: 0.5426998725753668 Accuracy: 74.0 %\n",
      "Epoch 159 done. Loss: 0.6048769506586239 Accuracy: 76.0 %\n",
      "Epoch 160 done. Loss: 0.7307852617463947 Accuracy: 66.0 %\n",
      "Epoch 161 done. Loss: 0.6461808732579503 Accuracy: 63.0 %\n",
      "Epoch 162 done. Loss: 0.4630291714857768 Accuracy: 68.0 %\n",
      "Epoch 163 done. Loss: 1.056730124335683 Accuracy: 78.0 %\n",
      "Epoch 164 done. Loss: 0.34830663865662015 Accuracy: 69.0 %\n",
      "Epoch 165 done. Loss: 0.6361347491951809 Accuracy: 80.0 %\n",
      "Epoch 166 done. Loss: 0.4530300582093782 Accuracy: 78.0 %\n",
      "Epoch 167 done. Loss: 0.456407474569355 Accuracy: 68.0 %\n",
      "Epoch 168 done. Loss: 0.8997259539984657 Accuracy: 75.0 %\n",
      "Epoch 169 done. Loss: 0.4793015372140407 Accuracy: 80.0 %\n",
      "Epoch 170 done. Loss: 0.4852056851205381 Accuracy: 75.0 %\n",
      "Epoch 171 done. Loss: 0.27773784653919664 Accuracy: 74.0 %\n",
      "Epoch 172 done. Loss: 0.8437947245401933 Accuracy: 73.0 %\n",
      "Epoch 173 done. Loss: 0.370425313786949 Accuracy: 73.0 %\n",
      "Epoch 174 done. Loss: 0.3340184591074075 Accuracy: 72.0 %\n",
      "Epoch 175 done. Loss: 0.738915933970774 Accuracy: 72.0 %\n",
      "Epoch 176 done. Loss: 0.12995258035896398 Accuracy: 72.0 %\n",
      "Epoch 177 done. Loss: 0.4507708506892115 Accuracy: 75.0 %\n",
      "Epoch 178 done. Loss: 0.6134082033382982 Accuracy: 72.0 %\n",
      "Epoch 179 done. Loss: 0.44840990174911705 Accuracy: 73.0 %\n",
      "Epoch 180 done. Loss: 0.6133856799124213 Accuracy: 75.0 %\n",
      "Epoch 181 done. Loss: 0.296277449452135 Accuracy: 75.0 %\n",
      "Epoch 182 done. Loss: 0.6660210288658444 Accuracy: 75.0 %\n",
      "Epoch 183 done. Loss: 0.43692025312388627 Accuracy: 77.0 %\n",
      "Epoch 184 done. Loss: 0.5928863071444974 Accuracy: 77.0 %\n",
      "Epoch 185 done. Loss: 0.6994221472784581 Accuracy: 72.0 %\n",
      "Epoch 186 done. Loss: 0.47551648580828004 Accuracy: 77.0 %\n",
      "Epoch 187 done. Loss: 0.7374663118777435 Accuracy: 78.0 %\n",
      "Epoch 188 done. Loss: 0.5906689612265237 Accuracy: 77.0 %\n",
      "Epoch 189 done. Loss: 0.7312726914515867 Accuracy: 73.0 %\n",
      "Epoch 190 done. Loss: 0.6286702027467089 Accuracy: 74.0 %\n",
      "Epoch 191 done. Loss: 0.7944504088582428 Accuracy: 77.0 %\n",
      "Epoch 192 done. Loss: 0.5241132672588356 Accuracy: 78.0 %\n",
      "Epoch 193 done. Loss: 0.5205775972296827 Accuracy: 69.0 %\n",
      "Epoch 194 done. Loss: 0.433068375624269 Accuracy: 75.0 %\n",
      "Epoch 195 done. Loss: 0.6039040465143363 Accuracy: 65.0 %\n",
      "Epoch 196 done. Loss: 0.8475355365475515 Accuracy: 73.0 %\n",
      "Epoch 197 done. Loss: 0.3854054665029488 Accuracy: 75.0 %\n",
      "Epoch 198 done. Loss: 0.5995847063899903 Accuracy: 77.0 %\n",
      "Epoch 199 done. Loss: 0.4544399088245554 Accuracy: 78.0 %\n"
     ]
    }
   ],
   "source": [
    "m = Model([175, 167], n_a2=7000, n_a3=2, d_kernel1=5, w_h_kernel1=5, d_kernel2=5, w_h_kernel2=5, poolingmethod=\"max\", w_h_pooling1=3, w_h_pooling2=3, stride_conv1=2, stride_conv2=2, stride_pooling1=1, stride_pooling2=1, learningrate=1, batchsize=27)\n",
    "m.train(200, \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//train\", \"C://Users//Benni//Desktop//PK//Dateset//yes_and_no//test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
